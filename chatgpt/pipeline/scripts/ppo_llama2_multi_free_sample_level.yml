##### 流程配置 #####
pipeline:
  policy_model_type: "llama2_13B"
  reward_model_type: "llama_13B"
  pipeline:
    # - prepare
    - ppo
  granularity: "token_mix_sample"
  # dataset_path: "/cognitive_comp/songzhuoyang/processed_data/ppopp_dataset_0815_for_llama2_13b.jsonl"
  dataset_path: "/cognitive_comp/songzhuoyang/processed_data/ppopp_dataset_0815_llama2_13b_single.jsonl"
  workspace_path: "/cognitive_comp/songzhuoyang/workspace/rlhf/PPO_LLAMA2_13B_Multitask_sample_level/0817_1031/"
  logging_path: "/cognitive_comp/songzhuoyang/workspace/rlhf/PPO_LLAMA2_13B_Multitask_sample_level/"
  logging_level: debug ### debug, info, warning, error
  enable_flops_profiler: false
  gpus: 4
  
##### 细节配置（可选） #####
logger:
  wandb_project: "PPO_LLAMA2_13B_Multitask_sample_level"
  wandb_name: "0817_1031-free-single"
  wandb_group: ""
  wandb_team: "yukawa"

megatron:
  deepspeed_stage: 2
  offload_optimizer: true
  policy_precision: bf16
  rm_precision: fp16
  tensor_model_parallel_size: 4
  pipe_model_parallel_size: 1
  seed: 42
  gradient_accumulation_steps: 8
  enable_hybrid_engine: false

generation:
  top_k: 0
  top_p: 1.0
  temperature: 1.0
  repetition_penalty: 1.0
  max_new_tokens: 1024

ppo:
  trainer:
    actor_lr: 5.0e-06
    critic_lr: 5.0e-05
    num_episodes: 512
    total_steps: 1024
    max_epoch_per_update: 2
    policy_train_batch_size: 2
    kl_coef: 0.0
    val_every_n_episode: 5
    val_size_per_task: 25
    critic_from_sft: true
  experience:
    experience_batch_size: 128
    generate_minibatch_size: 32
    policy_minibatch_size: 8
    rm_minibatch_size: 2
    sample_batch_size: 128
    buffer_limit_size: 512
    replay_buffer_cpu_offload: true
    sample_replay_buffer: false
  ppo_details:
    eps_clip: 0.2
    value_clip: 0.2
    gamma: 0.99
    lam: 0.95
    enable_gae: false
    enable_step_level: false
    enable_token_level_loss: false
    enable_reward_scaling: false
    enable_token_reward: false
    enable_mixed_sample_reward: true
  ppopp_details:
    ppopp_beta: 0.0
    ppopp_beta_decay: 1.0
    ppopp_rate: 0.9
    ppopp_rate_decay: 0.97
    use_guide_action: false

##### 默认配置 #####

defaults:
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-05
  learning_rate: 1.0e-06
  min_learning_rate: 1.0e-07
  lr_decay_ratio: 1.0
  lr_decay_steps: 0
  warmup_max_lr: 0.0001
  warmup_min_lr: 1.0e-09
  warmup_ratio: 0.01
  warmup_steps: 0
  weight_decay: 0.1
  scheduler_type: cosine

data:
  prefix: 
    model: "<bot>:"
    human: "<human>:"
  multiturn_seperator: "\n"  

policy_model:
  policy_model_type: "llama2_13B"
  policy_tokenizer_path: null
  policy_model_path: null
  policy_max_seq_len: 2048

reward_model:
  reward_model_type: "llama_13B"
  rm_tokenizer_path: null
  reward_model_path: "/cognitive_comp/liangyuxin/workspace/chatgpt/13B_RM/ckpt/RM_LLAMA_13B_0801_multi/global_step14424_hf"
  rm_max_seq_len: 2048

